<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Statistical Divergences</title>
  <meta name="description" content="This is a brief review of statistical divergences as explained in chapter 8 of Computational Optimal Transport. A divergence $D$ satisfies the following prop...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/machinelearning/2020/07/11/statistical-divergences.html">
  <link rel="alternate" type="application/rss+xml" title="Topics in Machine Learning." href="http://localhost:4000/feed.xml">

  <!-- <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          extensions: ["tex2jax.js"],
          jax: ["input/TeX", "output/HTML-CSS"],
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true
          },
      "HTML-CSS": { availableFonts: ["TeX"] }
        });
  </script> -->
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Topics in Machine Learning.</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Statistical Divergences</h1>
    <p class="post-meta"><time datetime="2020-07-11T10:56:12-07:00" itemprop="datePublished">Jul 11, 2020</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This is a brief review of statistical divergences as explained in chapter 8 of <a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a>. A <strong>divergence</strong> $D$ satisfies the following properties:</p>
<ul>
  <li>$D(\alpha, \beta) \ge 0$.</li>
  <li>$D(\alpha, \beta) = 0 \Longleftrightarrow \alpha = \beta$</li>
</ul>

<p>However, unlike a distance it does not have to be symmetric or satisfy the traingle inequality. Divergences compare two input measures by comparing their masses <em>pointwise</em>, without introducing any notion of mass transportation. Divergences are functionals which, by looking at the pointwise ratio between two measures, give a sense of how close they are. They have nice analytical and computational properties and build  upon <em>entropy functions</em>.</p>

<ul>
  <li>If ${\displaystyle X}$ is a topological space, ${\displaystyle x_{0}}$ is a point in ${\displaystyle X}$ and ${\displaystyle f\colon X\to \mathbb {R} \cup \{-\infty ,\infty \}}$ is an extended real-valued function. We say that ${\displaystyle f}$ is <strong>lower semi-continuous</strong> at ${\displaystyle x_{0}}$ if for every ${\displaystyle y&lt;f(x_{0})}$ there exists a neighborhood ${\displaystyle U}$ of ${\displaystyle x_{0}}$ such that ${\displaystyle y&lt;f(x)}$ for all ${\displaystyle x\in U}$. For the particular case of a metric space, this can be expressed as</li>
</ul>

\[{\displaystyle \liminf _{x\to x_{0}}f(x)\geq f(x_{0})}\]

<p>        The function ${\displaystyle f}$ is called lower semi-continuous if it is lower semi-continuous at every point of its 
              domain. A function is lower semi-continuous if and only if ${\displaystyle \{x\in X:~f(x)&gt;y \}}$ is an open set for 
          every ${\displaystyle y\in \mathbb {R} }$. Alternatively, a function is lower semi-continuous if and only if all of its lower level 
             sets ${\displaystyle \{x\in X:~f(x)\leq y\}}$ are closed. Lower level sets are also called sublevel sets or trenches.</p>

<ul>
  <li>If ${\displaystyle X}$ is a topological space, ${\displaystyle x_{0}}$ is a point in ${\displaystyle X}$ and ${\displaystyle f\colon X\to \mathbb {R} \cup \{-\infty ,\infty \}}$ is an extended real-valued function. We say that ${\displaystyle f}$ is <strong>upper semi-continuous</strong> at ${\displaystyle x_{0}}$ if for every ${\displaystyle y&gt;f(x_{0})}$ there exists a neighborhood ${\displaystyle U}$ of ${\displaystyle x_{0}}$ such that ${\displaystyle f(x)&lt;y}$ for all ${\displaystyle x\in U}$. For the particular case of a metric space, this can be expressed as</li>
</ul>

\[{\displaystyle \limsup _{x\to x_{0}}f(x)\leq f(x_{0})}\]

<p>         The function ${\displaystyle f}$ is called upper semi-continuous if it is upper semi-continuous at every point of its              domain. A function is upper semi-continuous if and only if ${\displaystyle \{x\in X:~f(x)&lt;y\}}$ is an open set                  for every ${\displaystyle y\in \mathbb {R} }$.</p>

<ul>
  <li>
    <p>A function is continuous at $x_0$ if and only if it is both upper and lower semi-continuous there.</p>
  </li>
  <li>
    <p>A function $\phi : \mathbb{R} \to \mathbb{R} \cup \{\infty\}$ is an <strong>entropy function</strong> if it is lower semicontinuous, convex, $\mathrm{dom} ~\phi\subset [0,\infty)$, and satisfies the following feasibility condition:  $\mathrm{dom} ~ \phi \; \cap   (0, \infty) \neq \emptyset$. The speed of growth of $\phi$ at $\infty$ is described by</p>
  </li>
</ul>

\[\phi'_\infty = \lim_{x\rightarrow +\infty} \frac{\varphi(x)}{x} \in \mathbb{R} \cup \{\infty\} \, .\]

<p>        If $\phi’_\infty = \infty$, then $\phi$ grows faster than any linear function and $\phi$ is said <em>superlinear</em>. Any  entropy 
              function $\phi$ induces a $\phi$-divergence or $f$-divergence) as defined below.</p>

<ul>
  <li>Let $\phi$ be an entropy function. For $\alpha,\beta \in \mathcal{M}(\mathcal{X})$, let $\frac{\mathrm{d} \alpha}{\mathrm{d} \beta} \beta + \alpha^{\perp}$ be the Lebesgue decomposition of $\alpha$ with respect to $\beta$. The <strong>divergence</strong> $\mathcal{D}_\phi$ is defined by</li>
</ul>

\[\mathcal{D}_\phi (\alpha|\beta) \triangleq \int_X \phi\left(\frac{\mathrm{d} \alpha}{\mathrm{d} \beta} \right) \mathrm{d} \beta + \phi'_\infty \alpha^{\perp}(X)\]

<p>        if $\alpha,\beta$ are nonnegative and $\infty$ otherwise.</p>

<ul>
  <li>In case of discrete measures $\alpha = \sum_i a_i \delta_{x_i}$ and $\beta = \sum_i b_i \delta_{x_i}$ which are supported on the same set of $n$ points $(x_i)_{i=1}^n \subseteq \mathcal{X}$ the <strong>$\phi$-divergence</strong> is defined as</li>
</ul>

\[\mathcal{D}_\phi(\mathbf{a}|\mathbf{b}) = \sum_{i \in \mathrm{Supp}(\mathbf{b})} \phi\left({ \frac{a_i}{b_i} } \right) b_i + \phi'_\infty \sum_{i \notin \mathrm{Supp}(\mathbf{b})} a_i,\]

<p>        where \(\mathrm{Supp}(b) \triangleq \{i \in \mathbb{[} n \mathbb{]} ~:~ b_i \neq 0 \}\).</p>

<ul>
  <li>Many divergences can be written as $\phi$-divergence. Here are a few examples:</li>
</ul>

<table class="td">
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: center">\(D_f(P \lVert Q)\)</th>
      <th style="text-align: center">Generator $\phi$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">KL-divergence</td>
      <td style="text-align: center">$\int p(x) \log \left( \frac{p(x)}{q(x)} \right) \mathrm{d}x$</td>
      <td style="text-align: center">${\displaystyle t\log t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">reverse KL-divergence</td>
      <td style="text-align: center">$\int q(x) \log \left( \frac{p(x)}{q(x)} \right) \mathrm{d}x$</td>
      <td style="text-align: center">${\displaystyle -\log t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">squared Hellinger distance</td>
      <td style="text-align: center">$\int \left( \sqrt{p(x)} - \sqrt{q(x)} \right) ^2 \mathrm{d} x$</td>
      <td style="text-align: center">$({\sqrt  {t}}-1)^{2},\,2(1-{\sqrt  {t}})$</td>
    </tr>
    <tr>
      <td style="text-align: left">Total variation distance</td>
      <td style="text-align: center">$\frac{1}{2} \int \lvert p(x) - q(x) \rvert \mathrm{d} x$</td>
      <td style="text-align: center">$\frac{1}{2} \lvert t - 1 \rvert$</td>
    </tr>
    <tr>
      <td style="text-align: left">Pearson $\chi ^{2}$-divergence</td>
      <td style="text-align: center">$\int \frac{(p(x) - q(x))^2}{p(x)}\mathrm{d} x$</td>
      <td style="text-align: center">${\displaystyle (t-1)^{2},\,t^{2}-1,\,t^{2}-t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Neyman $\chi ^{2}$-divergence</td>
      <td style="text-align: center">$\int \frac{(p(x) - q(x))^2}{q(x)}\mathrm{d} x$</td>
      <td style="text-align: center">${\displaystyle {\frac {1}{t}}-1,\,{\frac {1}{t}}-t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Jensen-Shannon divergence</td>
      <td style="text-align: center">\(\frac{1}{2} \int p(x) \log \left( \frac{2p(x)}{p(x) + q(x)} \right) +   q(x) \log \left( \frac{2q(x)}{p(x) + q(x)} \right)\mathrm{d} x\)</td>
      <td style="text-align: center">$-(1+t)\log \left( \frac{1+t}{2} \right) + t \log(t) $</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>A few more resources to read about $\phi$-divergences:
    <ul>
      <li><a href="http://www.stat.yale.edu/~yw562/teaching/it-stats.pdf">Yale university lecture notes on information-theoretic methods for high-dimensional statistics</a></li>
      <li><a href="http://people.lids.mit.edu/yp/homepage/data/LN_fdiv.pdf">MIT notes on f-divergences</a></li>
    </ul>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy"><strong>Renyi divergence</strong></a> is a generalization of KL-divergence by using the Renyi-entropy function which is a generalization of Shannon-entropy.</p>
  </li>
  <li>The following diagram shows some of the inequalities that hold between different divergences:
<img src="divergences-comparison.png" alt="Comparing divergences" /></li>
</ul>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Topics in Machine Learning.</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Topics in Machine Learning.</li>
          <li><a href="mailto:mohsen.kiskani@gmail.com">mohsen.kiskani@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/kiskani"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">kiskani</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>This page is dedicated to some interesting ideas in statistics and machine learning. I will try to scratch the surface about some interesting ideas. Mainly for my own benefit and access but maybe helpful for others too.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
