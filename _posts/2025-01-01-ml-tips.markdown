---
layout: post
title:  "Machine Learning tips"
date:   2025-01-01 00:00:00 -0000
categories: machinelearning
---

In this page, I will summarize a few tips related to training and use of machine learning models and techniques. 

# Dropout 

* Applying dropout to a neural network amounts to sampling a ``thinned'' network from
it. The thinned network consists of all the units that survived dropout. A
neural net with $n$ units, can be seen as a collection of $2^{n}$ possible thinned neural networks.
These networks all share weights so that the total number of parameters is still $O(n^2)$, or less. For each presentation of each training case, a new thinned network is sampled and
trained. So training a neural network with dropout can be seen as training a collection of $2^n$
thinned networks with extensive weight sharing, where each thinned network gets trained
very rarely, if at all.

* Under some key assumptions like operating in the linear region of a neural network where softmax in linear and with some very restrictive assumptions like linearity of log (close to 0) we can prove that the geometric mean of the softmax outputs of this ensemble of networks trained with dropout is equivalent to the softmax output of a single network with weights scaled by the dropout probability p at test time (ask Gemini for proving this also watch [here around 18:00[(https://www.youtube.com/watch?v=DleXA5ADG78]). 
