<p>In this post, I will explain some of the basic and fundamental theorems and concepts in estimation theory. Mostly, I am putting the summary of the great book of <a href="https://www.amazon.com/Fundamentals-Statistical-Signal-Processing-Estimation/dp/0133457117/">Steven Kay</a> in here.</p>

<h2 id="minimum-variance-unbiased-estimation">Minimum Variance Unbiased Estimation</h2>

<p>Assume that we want to estimate a parameter $\theta$ in a model. An estimation $\hat{\theta}$ is said to be <em>unbiased</em> if</p>

\[\mathbb{E}[\hat{\theta}] = \theta.\]

<p>If an estimation is not unbiased, then the value of <em>bias</em> for estimation is defined as</p>

\[b(\theta) \triangleq \mathbb{E}[\hat{\theta}] - \theta\]

<h3 id="minimum-variance-criterion">Minimum variance criterion</h3>

<p>A natural choice for the optimality criterion is <em>Mean Squared Error (MSE)</em> defined as</p>

\[mse(\hat{\theta}) \triangleq \mathbb{E}[(\hat{\theta} - \theta)^2] = var(\hat{\theta}) + b^2(\theta)\]

<p>Unfortunately, adoption of this natural choice leads to unrealizable estimators, ones that cannot be written solely as a function of data since it depends on bias. Hence, from a practical point of view, MSE estimators are useless. An alternative approach is to constrain the bias to zero and minimize the variance. Such an estimation is called <strong><em>Minimum Variance Unbiased (MVU)</em></strong> estimation. Unfortunately, there is also no guarantee that an MVU exists. If MVU exists, there are a few techniques that could help us find it.</p>
<ul>
  <li>Finding the <em>Cramer Rao Lower Bound (CRLB)</em> and checking if some estimator can achieve it.</li>
  <li>Applying the <em>Rao-Blackwell-Lehmann-Schaffe (RBLS)</em> theorem to an unbiased estimator.</li>
  <li>Finding the <em>Best Linear Unbiased Estimator (BLUE)</em>.</li>
</ul>

<p>CRLB is a lower bound for the variance of any unbiased estimator. If an estimator exists whose variance equals to that of CRLB <em>for any value of $\theta$</em>, then it must be the MVU estimator. In this case, the theory of CRLB immediately yields the estimator. It may happen that no estimator exists whose variance equals the bound, yet, MVU estimator may still exist. In such cases, we must resort to RBLS theorem to find the MVU. The third approach, BLUE, only works for particular datasets.</p>

<h2 id="cramer-rao-lower-bound-crlb">Cramer Rao Lower Bound (CRLB)</h2>
<p>Assume that the PDF function $p(x; \theta)$ satisfies the regularity condition</p>

\[\mathbb{E} \left[\frac{\partial \ln p(x;\theta)}{\partial \theta} \right] = 0,  \quad \forall \theta\]

<p>where the expectation is taken with respect to $p(x; \theta)$. Then the variance of any unbiased estimator $\hat{\theta}$ must satisfy</p>

\[var(\hat{\theta}) \ge -\frac{1}{\mathbb{E}\left[ \frac{\partial^2 \ln p(x; \theta)}{\partial \theta^2}\right]} = \frac{1}{\mathbb{E}\left[ \left( \frac{\partial \ln p(x;\theta)}{\partial \theta}\right)^2 \right]}\]

<p>where the derivation is evaluated at the true value of $\theta$ and the expectation is taken with respect to $p(x; \theta)$. Furthermore, an unbiased estimator may be found that attains the bound for all $\theta$ if and only if</p>

\[\frac{\partial \ln p(x; \theta)}{\partial \theta} = I(\theta) (g(x) - \theta)\]

<p>for some functions $g$ and $I$. The estimator which is the MVU estimator is $\hat{\theta} = g(x)$ and the minimum variance is $1/I(\theta)$.</p>

<p>An estimator which is unbiased and attains the CRLB is said to be <strong><em>efficient</em></strong>. The denominator of the lower bound in CRLB is called <strong><em>Fisher information</em></strong>. If it is desired to find the estimate for</p>

\[\alpha = g(\theta)\]

<p>then the CRLB is proved to be</p>

\[var(\hat{\alpha}) \ge -\frac{\left(\frac{\partial g}{\partial \theta}\right)^2}{\mathbb{E}\left[ \frac{\partial^2 \ln p(x; \theta)}{\partial \theta^2}\right]}\]

<p>This shows that the efficiency is destroyed by a non-linear transformation like $g(\theta)$ while it is maintained by linear transformations.</p>

<h2 id="rao-blackwell-lehmann-schaffe-theorem">Rao-Blackwell-Lehmann-Schaffe Theorem</h2>

<p>RBLS Theorem allows us to find MVU estimator by a simple inspection of the PDF function.</p>

<h1 id="sufficient-statistics">Sufficient Statistics</h1>

<p>A reasonable question to ask is which data samples are pertinent to the estimation problem? There may be many sufficient data sets to find the MVU. However, the one with the least number of elements is called the <strong>minimal sufficient statistic</strong>. The Neyman-Fisher theorem which is used to find the sufficient statistics is stated as follows.</p>

<h1 id="neyman-fisher-factorization-theorem">Neyman-Fisher Factorization Theorem</h1>

<p>If we can factor the pdf $p(x; \theta)$ as</p>

\[p(x;\theta) = g(T(x), \theta) h(x)
\label{eq_fac}\]

<p>where $g$ is a function depending on $x$ only through $T(x)$ and $h$ is a function depending only on $x$, then $T(x)$ is a sufficient statistic for $\theta$. Conversely, if $T(x)$ is a sufficient statistic for $\theta$, then the pdf can be factored as in \eqref{eq_fac}.</p>

<p>Using the Neyman-Fisher theorem we can find sufficient statistics. Then we will use the RBLS theorem as to find MVU as follows.</p>

<h1 id="rbls-theorem">RBLS Theorem</h1>

<p>If $\check{\theta}$ is an unbiased estimator of $\theta$ and $T(x)$ is a sufficient statistic for $\theta$, then</p>

\[\hat{\theta} = \mathbb{E}\left[ \check{\theta} | T(x) \right]\]

<p>is</p>
<ul>
  <li>a valid estimator for $\theta$ (not depending on $\theta$)</li>
  <li>unbiased</li>
  <li>of lesser or equal variance than that of $\check{\theta}$, for all $\theta$.</li>
</ul>

<p>Additionally, if the sufficient statistic is <strong><em>complete</em></strong>, then $\hat{\theta}$ is the MVU estimator. A statistic is <em>complete</em> if there is only one function of the statistic that is unbiased. The property of completeness depends on the pdf of $x$, which in turn determines the pdf of the sufficient statistic. For many practical cases of interest this property holds. For instance, it holds for the exponential family of distributions.</p>

<h1 id="rao-blackwellization">Rao Blackwellization</h1>

<p>Rao Blackwellization is a procedure to find the MVU estimator using the RBLS theorem. It involves the following steps:</p>
<ol>
  <li>Use Neyman-Fisher factorization theorem to find a single sufficient statistic $T(x)$ for $\theta$.</li>
  <li>Determine if the sufficient statistic is complete and if so, proceed. If not, this approach cannot be used to find MVU.</li>
  <li>Find a function $g$ of the sufficient statistic that yields an unbiased estimator $\hat{\theta} = g(T(x))$. This will be the MVU estimator.</li>
</ol>

<p>As an alternative implementation of step 3, we may evaluate 
\(\hat{\theta} = \mathbb{E}\left[ \check{\theta} | T(x) \right]\), where $\check{\theta}$ is any unbiased estimator.</p>

<h2 id="best-linear-unbiased-estimator-blue">Best Linear Unbiased Estimator (BLUE)</h2>

<p>The BLUE restricts the estimator to be linear in data</p>

\[\hat{\theta} = \sum_{i=0}^{N-1} a_i x[i]\]

<p>where $a_i$’s are constants yet to be determined. The unbiased constraint is written as</p>

\[\mathbb{E}[\hat{\theta}] = \sum_{i=0}^{N-1} a_i \mathbb{E}[x[i]] = \theta\]

<p>In order to satisfy this constraint, $\mathbb{E}[x[i]]$ must be linear in $\theta$ or</p>

\[\mathbb{E}[x[i]] = s[i] \theta\]

<p>where the $s[i]$’s are known. Therefore, the unbiased contraint can be written as</p>

\[\mathbf{a}^T \mathbf{s} = 1
\label{eq_bias}\]

<p>We also have</p>

\[var(\hat{\theta}) = \mathbb{E}\left[\mathbf{a}^T (\mathbf{x} - \mathbb{E}[\mathbf{x}])(\mathbf{x} - \mathbb{E}[\mathbf{x}])^T\mathbf{a} \right]= \mathbf{a}^T \mathbf{C} \mathbf{a}
\label{eq_var}\]

<p>If we minimize variance in \eqref{eq_var} subject to \eqref{eq_bias}, we can find the optimal coefficients $\mathbf{a}$ as</p>

\[\mathbf{a}_{opt} = \frac{\mathbf{C}^{-1}\mathbf{s}}{\mathbf{s}^T\mathbf{C}^{-1}\mathbf{s}}\]

<p>Hence, the BLUE is</p>

\[\hat{\theta} = \frac{\mathbf{s}^T\mathbf{C}^{-1}\mathbf{x}}{\mathbf{s}^T\mathbf{C}^{-1}\mathbf{s}}\]

<p>and the minimum variance is equal to</p>

\[var(\hat{\theta}) = \frac{1}{\mathbf{s}^T\mathbf{C}^{-1}\mathbf{s}}\]

<h2 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h2>

<p>MLE is an estimation of the value of $\theta$ that maximizes the value of $p(x; \theta)$ the likelihood function for fixed $x$. The maximization is done over the range of all allowable values for $\theta$. It is used in situations that MVU does not exist or cannot be found even if it does exist. It is the most popular “turn-the-crank” estimation procedure. For most cases practical intereste it’s performance is optimal for large enough data records. It is an estimator that is <strong>approximately optimal</strong>. It can be proved that as $N \to \infty$, the proposed estimator is unbiased and efficient. Hence, it is <strong>aymptotically unbiased</strong> and <strong>asymptotically efficient</strong>. More strongly, we can prove that if the pdf $p(x; \theta)$ satisfies some “regularity” conditions, then the MLE of the unknown parameter $\theta$ is asymptotically distributed (for large data records) according to</p>

\[\hat{\theta} \overset{\text{a}}{\sim} \mathcal{N}(\theta, I^{-1}(\theta))\]

<p>where $I(\theta)$ is the Fisher information evaluated at the true value of the unknown parameter $\theta$. This shows that not only the estimator is asymptotically unbiased and can asymptotically achieve the CRLB (due to law of large numbers) but also it is following a Gaussian distribution.</p>

<p>The MLE estimation also follows <strong>invariance property</strong> which means that the MLE of the transformation of a parameter is found by substituting the MLE of the original parameter into the transformation operation. More precisely, the MLE of the parameter</p>

\[\alpha = g(\theta)\]

<p>where the pdf $p(x; \theta)$ is parametrized by $\theta$, is given by</p>

\[\hat{\alpha} = g(\hat{\theta})\]

<p>where $\hat{\theta}$ is the MLE of $\theta$ and $\hat{\theta}$ is found by maximizing $p(x;\theta)$. If $g$ is not a one-to-one function, then $\hat{\alpha}$ maximizes the modified likelihood function</p>

\[p_T(x; \alpha) \triangleq \max_{\{\theta: \alpha = g(\theta)\}} p(x;\theta)\]

<p>Another very important property of MLE estimation is that MLE is equivalent to minimizing the KL distance between our estimate and the real data distribution. For a proof of this important fact, see <a href="https://wiseodd.github.io/techblog/2017/01/26/kl-mle/">here</a>.</p>

<!---
## Bayesian Estimation 

In situations when MVU estimator cannot be found, we can use a Bayesian approach in which $\theta$ is considered to be a random variable as opposed to a deterministic value that we are interested in finding. 


<figure class="highlight"><pre><code class="language-python" data-lang="python"> 
<span class="n">pd</span><span class="p">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_rows'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span></code></pre></figure>


Useful resources:

[https://readthedocs.org/projects/pandas-datareader/downloads/pdf/latest/](https://readthedocs.org/projects/pandas-datareader/downloads/pdf/latest/)
--->
