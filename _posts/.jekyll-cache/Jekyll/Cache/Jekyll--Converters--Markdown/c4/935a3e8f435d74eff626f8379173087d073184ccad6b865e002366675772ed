I"Õ<p>This is a brief review of statistical divergences as explained in chapter 8 of <a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a>. A <strong>divergence</strong> $D$ satisfies the following properties:</p>
<ul>
  <li>$D(\alpha, \beta) \ge 0$.</li>
  <li>$D(\alpha, \beta) = 0 \Longleftrightarrow \alpha = \beta$</li>
</ul>

<p>However, unlike a distance it does not have to be symmetric or satisfy the traingle inequality. Divergences compare two input measures by comparing their masses <em>pointwise</em>, without introducing any notion of mass transportation. Divergences are functionals which, by looking at the pointwise ratio between two measures, give a sense of how close they are. They have nice analytical and computational properties and build  upon <em>entropy functions</em>.</p>

<ul>
  <li>If ${\displaystyle X}$ is a topological space, ${\displaystyle x_{0}}$ is a point in ${\displaystyle X}$ and ${\displaystyle f\colon X\to \mathbb {R} \cup \{-\infty ,\infty \}}$ is an extended real-valued function. We say that ${\displaystyle f}$ is <strong>lower semi-continuous</strong> at ${\displaystyle x_{0}}$ if for every ${\displaystyle y&lt;f(x_{0})}$ there exists a neighborhood ${\displaystyle U}$ of ${\displaystyle x_{0}}$ such that ${\displaystyle y&lt;f(x)}$ for all ${\displaystyle x\in U}$. For the particular case of a metric space, this can be expressed as</li>
</ul>

\[{\displaystyle \liminf _{x\to x_{0}}f(x)\geq f(x_{0})}\]

<p>Â  Â  Â  Â  The function ${\displaystyle f}$ is called lower semi-continuous if it is lower semi-continuous at every point of its 
Â  Â  Â  Â  Â   Â   domain. A function is lower semi-continuous if and only if ${\displaystyle \{x\in X:~f(x)&gt;y \}}$ is an open set for 
Â  Â  Â  Â  Â  every ${\displaystyle y\in \mathbb {R} }$. Alternatively, a function is lower semi-continuous if and only if all of its lower level 
Â  Â  Â  Â  Â  Â   sets ${\displaystyle \{x\in X:~f(x)\leq y\}}$ are closed. Lower level sets are also called sublevel sets or trenches.</p>

<ul>
  <li>If ${\displaystyle X}$ is a topological space, ${\displaystyle x_{0}}$ is a point in ${\displaystyle X}$ and ${\displaystyle f\colon X\to \mathbb {R} \cup \{-\infty ,\infty \}}$ is an extended real-valued function. We say that ${\displaystyle f}$ is <strong>upper semi-continuous</strong> at ${\displaystyle x_{0}}$ if for every ${\displaystyle y&gt;f(x_{0})}$ there exists a neighborhood ${\displaystyle U}$ of ${\displaystyle x_{0}}$ such that ${\displaystyle f(x)&lt;y}$ for all ${\displaystyle x\in U}$. For the particular case of a metric space, this can be expressed as</li>
</ul>

\[{\displaystyle \limsup _{x\to x_{0}}f(x)\leq f(x_{0})}\]

<p>Â  Â  Â  Â   The function ${\displaystyle f}$ is called upper semi-continuous if it is upper semi-continuous at every point of its Â  Â  Â  Â  Â     domain. A function is upper semi-continuous if and only if ${\displaystyle \{x\in X:~f(x)&lt;y\}}$ is an open set Â  Â  Â  Â  Â   Â   Â   for every ${\displaystyle y\in \mathbb {R} }$.</p>

<ul>
  <li>
    <p>A function is continuous at $x_0$ if and only if it is both upper and lower semi-continuous there.</p>
  </li>
  <li>
    <p>A function $\phi : \mathbb{R} \to \mathbb{R} \cup \{\infty\}$ is an <strong>entropy function</strong> if it is lower semicontinuous, convex, $\mathrm{dom} ~\phi\subset [0,\infty)$, and satisfies the following feasibility condition:  $\mathrm{dom} ~ \phi \; \cap   (0, \infty) \neq \emptyset$. The speed of growth of $\phi$ at $\infty$ is described by</p>
  </li>
</ul>

\[\phi'_\infty = \lim_{x\rightarrow +\infty} \frac{\varphi(x)}{x} \in \mathbb{R} \cup \{\infty\} \, .\]

<p>Â  Â  Â  Â  If $\phiâ€™_\infty = \infty$, then $\phi$ grows faster than any linear function and $\phi$ is said <em>superlinear</em>. Any  entropy 
Â  Â  Â  Â   Â  Â   function $\phi$ induces a $\phi$-divergence or $f$-divergence) as defined below.</p>

<ul>
  <li>Let $\phi$ be an entropy function. For $\alpha,\beta \in \mathcal{M}(\mathcal{X})$, let $\frac{\mathrm{d} \alpha}{\mathrm{d} \beta} \beta + \alpha^{\perp}$ be the Lebesgue decomposition of $\alpha$ with respect to $\beta$. The <strong>divergence</strong> $\mathcal{D}_\phi$ is defined by</li>
</ul>

\[\mathcal{D}_\phi (\alpha|\beta) \triangleq \int_X \phi\left(\frac{\mathrm{d} \alpha}{\mathrm{d} \beta} \right) \mathrm{d} \beta + \phi'_\infty \alpha^{\perp}(X)\]

<p>Â  Â  Â  Â  if $\alpha,\beta$ are nonnegative and $\infty$ otherwise.</p>

<ul>
  <li>In case of discrete measures $\alpha = \sum_i a_i \delta_{x_i}$ and $\beta = \sum_i b_i \delta_{x_i}$ which are supported on the same set of $n$ points $(x_i)_{i=1}^n \subseteq \mathcal{X}$ the <strong>$\phi$-divergence</strong> is defined as</li>
</ul>

\[\mathcal{D}_\phi(\mathbf{a}|\mathbf{b}) = \sum_{i \in \mathrm{Supp}(\mathbf{b})} \phi\left({ \frac{a_i}{b_i} } \right) b_i + \phi'_\infty \sum_{i \notin \mathrm{Supp}(\mathbf{b})} a_i,\]

<p>Â  Â  Â  Â  where \(\mathrm{Supp}(b) \triangleq \{i \in \mathbb{[} n \mathbb{]} ~:~ b_i \neq 0 \}\).</p>

<ul>
  <li>Many divergences can be written as $\phi$-divergence. Here are a few examples:</li>
</ul>

<table class="td">
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: center">\(D_f(P \lVert Q)\)</th>
      <th style="text-align: center">Generator $\phi$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">KL-divergence</td>
      <td style="text-align: center">$\int p(x) \log \left( \frac{p(x)}{q(x)} \right) \mathrm{d}x$</td>
      <td style="text-align: center">${\displaystyle t\log t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">reverse KL-divergence</td>
      <td style="text-align: center">$\int q(x) \log \left( \frac{p(x)}{q(x)} \right) \mathrm{d}x$</td>
      <td style="text-align: center">${\displaystyle -\log t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">squared Hellinger distance</td>
      <td style="text-align: center">$\int \left( \sqrt{p(x)} - \sqrt{q(x)} \right) ^2 \mathrm{d} x$</td>
      <td style="text-align: center">$({\sqrt  {t}}-1)^{2},\,2(1-{\sqrt  {t}})$</td>
    </tr>
    <tr>
      <td style="text-align: left">Total variation distance</td>
      <td style="text-align: center">$\frac{1}{2} \int \lvert p(x) - q(x) \rvert \mathrm{d} x$</td>
      <td style="text-align: center">$\frac{1}{2} \lvert t - 1 \rvert$</td>
    </tr>
    <tr>
      <td style="text-align: left">Pearson $\chi ^{2}$-divergence</td>
      <td style="text-align: center">$\int \frac{(p(x) - q(x))^2}{p(x)}\mathrm{d} x$</td>
      <td style="text-align: center">${\displaystyle (t-1)^{2},\,t^{2}-1,\,t^{2}-t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Neyman $\chi ^{2}$-divergence</td>
      <td style="text-align: center">$\int \frac{(p(x) - q(x))^2}{q(x)}\mathrm{d} x$</td>
      <td style="text-align: center">${\displaystyle {\frac {1}{t}}-1,\,{\frac {1}{t}}-t}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Jensen-Shannon divergence</td>
      <td style="text-align: center">\(\frac{1}{2} \int p(x) \log \left( \frac{2p(x)}{p(x) + q(x)} \right) +   q(x) \log \left( \frac{2q(x)}{p(x) + q(x)} \right)\mathrm{d} x\)</td>
      <td style="text-align: center">$-(1+t)\log \left( \frac{1+t}{2} \right) + t \log(t) $</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>A few more resources to read about $\phi$-divergences:
    <ul>
      <li><a href="http://www.stat.yale.edu/~yw562/teaching/it-stats.pdf">Yale university lecture notes on information-theoretic methods for high-dimensional statistics</a></li>
      <li><a href="http://people.lids.mit.edu/yp/homepage/data/LN_fdiv.pdf">MIT notes on f-divergences</a></li>
    </ul>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy"><strong>Renyi divergence</strong></a> is a generalization of KL-divergence by using the Renyi-entropy function which is a generalization of Shannon-entropy.</p>
  </li>
  <li>The following diagram shows some of the inequalities that hold between different divergences:
<img src="divergences-comparison.png" alt="Comparing divergences" /></li>
</ul>
:ET